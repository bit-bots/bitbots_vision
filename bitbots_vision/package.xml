<package format="2">
  <name>bitbots_vision</name>
  <description>
    This is the vision package of the Hamburg Bit-Bots.

    The vision is able to detect lines, the field itself, the field boundary, goal posts, teammates, enemies and other obstacles.
    Its architecture is modular allowing easy implementation of new approaches resulting in a high grade of customizability.

    For ball detection, you can choose between an fcnn or multiple yolo implementations.

    The goalpost detection also runs via yolo or a conventional detection method,
    which is also used for obstacle and robot detection.

    The whole system is embedded in the ROS environment and
    able to run on many devices including the Nvidia Jetson TX2 in our Wolfgang robots.

    In the context of the Hamburg Bit-Bots, the Images are provided by a Basler industry grade ethernet camera.

    The camera drivers are not included in this package but can be auto launched.

    If you want the vision to run without starting a camera driver simply set the cli launch parameter ``camera:=false``.

    Every image source, that publishes a ``sensor_msgs/Image messages`` message is also supported.

    The ROS topics and many other parameters are defined in the ``visioparams.yaml`` config file.
    All used parameters are also changeable during run-time using ros dynamic reconfigure.

    For simulation usage, different parameters can be defined in the ``simparam.yaml`` which overrides the normal params.

    The debug mode with special debug output can be activated using ``debug:=true``.

    In ``bitbots_vision/models`` the neural network models are stored. These models are not part of this repository.
    Bit-Bots use the pull_data script in bitbots_meta.

    For the field detection which is needed for the field boundary and
    obstacle detection the vision uses RGB lookup table color spaces provided by the wolves_color_picker.

    These color spaces can be improved and converted to a pickle file for faster loading times using the colorspace_tool in the vision tools.

    The field color space itself can be dynamically adapted in real-time using the dynamic color space heuristic.

    Therefore the vision gets more resistant in natural light conditions.

  </description>
  <version>1.1.4</version>

  <maintainer email="7vahl@informatik.uni-hamburg.de">Florian Vahl</maintainer>
  <maintainer email="7gutsche@informatik.uni-hamburg.de">Jan Gutsche</maintainer>
  <maintainer email="info@bit-bots.de">Hamburg Bit-Bots</maintainer>

  <author email="7vahl@informatik.uni-hamburg.de">Florian Vahl</author>
  <author email="7gutsche@informatik.uni-hamburg.de">Jan Gutsche</author>
  <author email="8hbrandt@informatik.uni-hamburg.de">Hendrik Brandt</author>
  <author email="git@nfiedler.de">Niklas Fiedler</author>
  <author email="info@bit-bots.de">Hamburg Bit-Bots</author>

  <license>MIT</license>

  <buildtool_depend>catkin</buildtool_depend>
  <build_depend>message_generation</build_depend>
  <depend>rospy</depend>
  <depend>python-rospkg</depend>
  <depend>sensor_msgs</depend>
  <depend>std_msgs</depend>
  <depend>geometry_msgs</depend>
  <depend>trajectory_msgs</depend>
  <depend>image_transport</depend>
  <depend>message_runtime</depend>
  <depend>nav_msgs</depend>
  <depend>humanoid_league_speaker</depend>
  <depend>humanoid_league_msgs</depend>
  <depend>bitbots_msgs</depend>
  <depend>dynamic_reconfigure</depend>
  <depend>wolves_image_provider</depend>
  <depend>python3-numpy</depend>
  <depend>python3-opencv</depend>
  <depend>python3-tensorflow</depend>
  <depend>pylon_camera</depend>
  <depend>white_balancer</depend>
  <depend>bitbots_bringup</depend>
  <depend>bitbots_docs</depend>
  <depend>tf2</depend>


  <export>
    <bitbots_documentation>
      <language>python3</language>
      <status>unknown</status>
    </bitbots_documentation>
  </export>
</package>
